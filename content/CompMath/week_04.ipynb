{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm and distance\n",
    "\n",
    "#### Norm\n",
    "\n",
    "The *Euclidean norm* of an $n$-vector $x$ (named after the Greek mathematician Euclid), denoted $\\|x\\|$, is the squareroot of the sum of the squares of its elements,\n",
    "$$\n",
    "\\|x\\|=\\sqrt{x_1^2+x_2^2+\\cdots+x_n^2} .\n",
    "$$\n",
    "The Euclidean norm can also be expressed as the squareroot of the inner product of the vector with itself, i.e., $\\|x\\|=\\sqrt{x^T x}$.\n",
    "\n",
    "#### Properties of norm.\n",
    "\n",
    "Some important properties of the Euclidean norm are given below. Here $x$ and $y$ are vectors of the same size, and $\\beta$ is a scalar.\n",
    "\n",
    "- Nonnegative homogeneity. $\\|\\beta x\\|=|\\beta|\\|x\\|$. Multiplying a vector by a scalar multiplies the norm by the absolute value of the scalar.\n",
    "- Triangle inequality. $\\|x+y\\| \\leq\\|x\\|+\\|y\\|$. The Euclidean norm of a sum of two vectors is no more than the sum of their norms. (The name of this property will be explained later.) Another name for this inequality is subadditivity.\n",
    "- Nonnegativity. $\\|x\\| \\geq 0$.\n",
    "- Definiteness. $\\|x\\|=0$ only if $x=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "\n",
    "x = np.array([2, -1, 2])\n",
    "\n",
    "print(f\"Norm of x: {npl.norm(x)}\")\n",
    "print(f\"Square root of the inner product of x with itself: {np.sqrt(np.inner(x, x))}\")\n",
    "print(\n",
    "    f\"Square root of the sum of the squares of the elements of x: {np.sqrt(np.sum(np.array(x) ** 2))}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10)\n",
    "y = np.random.randn(10)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "print(f\"Norm of x + y: {npl.norm(x + y)}\")\n",
    "print(f\"Norm of x + Norm of y: {npl.norm(x) + npl.norm(y)}\")\n",
    "\n",
    "# the triangle inequality\n",
    "print(npl.norm(x + y) <= npl.norm(x) + npl.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root-mean-square value.\n",
    "\n",
    "The norm is related to the *root-mean-square* (RMS) value of an $n$-vector $x$, defined as\n",
    "$$\n",
    "\\operatorname{rms}(x)=\\sqrt{\\frac{x_1^2+\\cdots+x_n^2}{n}}=\\frac{\\|x\\|}{\\sqrt{n}} .\n",
    "$$\n",
    "The argument of the squareroot in the middle expression is called the mean square value of $x$, denoted $\\mathbf{m s}(x)$, and the RMS value is the squareroot of the mean square value. \n",
    "\n",
    "The RMS value of a vector $x$ is useful when comparing norms of vectors with different dimensions; the RMS value tells us what a 'typical' value of $\\left|x_i\\right|$ is. \n",
    "\n",
    "> For example, the norm of $\\mathbf{1}$, the $n$-vector of all ones, is $\\sqrt{n}$, but its RMS value is 1 , independent of $n$. More generally, if all the entries of a vector are the same, say, $\\alpha$, then the RMS value of the vector is $|\\alpha|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(x):\n",
    "    return npl.norm(x) / np.sqrt(len(x))\n",
    "\n",
    "\n",
    "t = np.linspace(0, 1, 101)\n",
    "x = np.cos(8 * t) - 2 * np.sin(11 * t)\n",
    "\n",
    "print(\"average:\", np.average(x))\n",
    "print(\"root mean square:\", rms(x))\n",
    "print(\"x:\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance\n",
    "\n",
    "#### Euclidean distance.\n",
    "\n",
    "We can use the norm to define the Euclidean distance between two vectors $a$ and $b$ as the norm of their difference:\n",
    "$$\n",
    "\\operatorname{dist}(a, b)=\\|a-b\\|.\n",
    "$$\n",
    "For one, two, and three dimensions, this distance is exactly the usual distance between points with coordinates $a$ and $b$. But the Euclidean distance is defined for vectors of any dimension; we can refer to the distance between two vectors of dimension 100. Since we only use the Euclidean norm in this book, we will refer to the Euclidean distance between vectors as, simply, the distance between the vectors. If $a$ and $b$ are $n$-vectors, we refer to the RMS value of the difference, $\\|a-b\\| / \\sqrt{n}$, as the $R M S$ deviation between the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "> ##### Feature distance\n",
    ">\n",
    "> If $x$ and $y$ represent vectors of $n$ features of two objects, the quantity $\\|x-y\\|$ is called the feature distance, and gives a measure of how different the objects are (in terms of their feature values). Suppose for example the feature vectors are associated with patients in a hospital, with entries such as weight, age, presence of chest pain, difficulty breathing, and the results of tests. We can use feature vector distance to say that one patient case is near another one (at least in terms of their feature vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "# Number of patients\n",
    "n_patients = 100\n",
    "\n",
    "# Generate random features for patients\n",
    "# Features: [weight (kg), age (years), chest pain (0 or 1),\n",
    "# difficulty breathing (0 or 1), test result (0-100)]\n",
    "\n",
    "weights = np.random.normal(70, 15, n_patients)  # Normal distribution around 70 kg\n",
    "ages = np.random.randint(20, 80, n_patients)  # Ages between 20 and 80\n",
    "chest_pain = np.random.randint(0, 2, n_patients)  # 0 or 1\n",
    "difficulty_breathing = np.random.randint(0, 2, n_patients)  # 0 or 1\n",
    "test_results = np.random.uniform(0, 100, n_patients)  # Test results between 0 and 100\n",
    "\n",
    "# Combine features into a feature matrix\n",
    "patient_data = np.column_stack(\n",
    "    (weights, ages, chest_pain, difficulty_breathing, test_results)\n",
    ")\n",
    "# patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate feature distance between two patients\n",
    "patient_1 = patient_data[0]  # First patient\n",
    "patient_2 = patient_data[1]  # Second patient\n",
    "patient_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (L2 norm) between the two patients\n",
    "feature_distance = np.linalg.norm(patient_1 - patient_2)\n",
    "\n",
    "print(f\"Feature distance between Patient 1 and Patient 2: {feature_distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the patients in a 2D space (using weight and age for simplicity)\n",
    "plt.scatter(patient_data[:, 0], patient_data[:, 1], alpha=0.6, label=\"Patients\")\n",
    "\n",
    "# Highlight the two patients\n",
    "plt.scatter(patient_1[0], patient_1[1], color=\"red\", label=\"Patient 1\", s=100)\n",
    "plt.scatter(patient_2[0], patient_2[1], color=\"blue\", label=\"Patient 2\", s=100)\n",
    "\n",
    "# Draw a line between the two patients\n",
    "plt.plot(\n",
    "    [patient_1[0], patient_2[0]],\n",
    "    [patient_1[1], patient_2[1]],\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Patient Feature Vectors\")\n",
    "plt.xlabel(\"Weight (kg)\")\n",
    "plt.ylabel(\"Age (years)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ##### RMS prediction error\n",
    ">\n",
    "> Suppose that the $n$-vector $y$ represents a time series of some quantity, for example, hourly temperature at some location, and $\\hat{y}$ is another $n$-vector that represents an estimate or prediction of the time series $y$, based on other information. The difference $y-\\hat{y}$ is called the prediction error, and its RMS value $\\mathbf{r m s}(y-\\hat{y})$ is called the _$R M S$ prediction error_. If this value is small (say, compared to rms $(y)$ ) the prediction is good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature_data(n, noise_level):\n",
    "    # Generate time points\n",
    "    time = np.arange(n)\n",
    "    # Create a sinusoidal temperature pattern\n",
    "    temperature = 20 + 10 * np.sin(2 * np.pi * time / 24)  # Daily cycle\n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, noise_level, n)\n",
    "    return temperature + noise\n",
    "\n",
    "\n",
    "def predict_temperature(n):\n",
    "    # Simple prediction: average temperature over a day\n",
    "    return 20 + 10 * np.sin(2 * np.pi * np.arange(n) / 24)\n",
    "\n",
    "\n",
    "def rms_error(y, y_hat):\n",
    "    return np.sqrt(np.mean((y - y_hat) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_hours = 72  # 3 days of hourly data\n",
    "noise_level = 2.0  # Standard deviation of noise\n",
    "\n",
    "# Generate synthetic temperature data\n",
    "actual_temperature = generate_temperature_data(n_hours, noise_level)\n",
    "predicted_temperature = predict_temperature(n_hours)\n",
    "\n",
    "# Calculate RMS prediction error\n",
    "error = rms_error(actual_temperature, predicted_temperature)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_temperature, label=\"Actual Temperature\", color=\"blue\")\n",
    "plt.plot(\n",
    "    predicted_temperature, label=\"Predicted Temperature\", color=\"orange\", linestyle=\"--\"\n",
    ")\n",
    "plt.title(\"Actual vs Predicted Temperature\")\n",
    "plt.xlabel(\"Hour\")\n",
    "plt.ylabel(\"Temperature (°C)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.axhline(\n",
    "    y=np.mean(actual_temperature),\n",
    "    color=\"green\",\n",
    "    linestyle=\":\",\n",
    "    label=\"Mean Temperature\",\n",
    ")\n",
    "plt.text(\n",
    "    0, np.mean(actual_temperature) + 1, f\"RMS Error: {error:.2f} °C\", color=\"green\"\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ##### Document dissimilarity\n",
    ">\n",
    "> Suppose $n$-vectors $x$ and $y$ represent the histograms of word occurrences for two documents. Then $\\|x-y\\|$ represents a measure of the dissimilarity of the two documents. We might expect the dissimilarity to be smaller when the two documents have the same genre, topic, or author; we would expect it to be larger when they are on different topics, or have different authors. \n",
    "> \n",
    "> As an example we form the word count histograms for the 5 Wikipedia articles with titles 'Veterans Day', 'Memorial Day', 'Academy Awards', 'Golden Globe Awards', and 'Super Bowl', using a dictionary of 4423 words.\n",
    ">\n",
    "> |  | Veterans <br> Day | Memorial <br> Day | Academy <br> Awards | Golden Globe <br> Awards | Super Bowl |\n",
    "> | :--- | :---: | :---: | :---: | :---: | :---: |\n",
    "> | Veterans Day | 0 | 0.095 | 0.130 | 0.153 | 0.170 |\n",
    "> | Memorial Day | 0.095 | 0 | 0.122 | 0.147 | 0.164 |\n",
    "> | Academy A. | 0.130 | 0.122 | 0 | 0.108 | 0.164 |\n",
    "> | Golden Globe A. | 0.153 | 0.147 | 0.108 | 0 | 0.181 |\n",
    "> | Super Bowl | 0.170 | 0.164 | 0.164 | 0.181 | 0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_titles = [\n",
    "    \"Veterans Day\",\n",
    "    \"Memorial Day\",\n",
    "    \"Academy Awards\",\n",
    "    \"Golden Globe Awards\",\n",
    "    \"Super Bowl\",\n",
    "]\n",
    "\n",
    "dissimilarity_matrix = np.array(\n",
    "    [\n",
    "        [0.0, 0.095, 0.130, 0.153, 0.170],  # Veterans Day\n",
    "        [0.095, 0.0, 0.122, 0.147, 0.164],  # Memorial Day\n",
    "        [0.130, 0.122, 0.0, 0.108, 0.164],  # Academy Awards\n",
    "        [0.153, 0.147, 0.108, 0.0, 0.181],  # Golden Globe Awards\n",
    "        [0.170, 0.164, 0.164, 0.181, 0.0],  # Super Bowl\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the dissimilarities\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.imshow(\n",
    "    dissimilarity_matrix,\n",
    "    cmap=\"coolwarm\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "plt.title(\"Document Dissimilarities\")\n",
    "plt.xticks(np.arange(5), article_titles, rotation=90)\n",
    "plt.yticks(np.arange(5), article_titles)\n",
    "plt.xlabel(\"Document Index\")\n",
    "plt.ylabel(\"Document Index\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Nearest neighbor\n",
    "\n",
    "> Suppose $z_1, \\ldots, z_m$ is a collection of $m n$-vectors, and that $x$ is another $n$-vector. We say that $z_j$ is the nearest neighbor of $x$ (among $\\left.z_1, \\ldots, z_m\\right)$ if\n",
    ">$$\n",
    ">\\left\\|x-z_j\\right\\| \\leq\\left\\|x-z_i\\right\\|, \\quad >i=1, \\ldots, m .\n",
    ">$$\n",
    ">**In words:** $z_j$ is the closest vector to $x$ among the vectors $z_1, \\ldots, z_m$. The idea of nearest neighbor, and generalizations such as the $k$-nearest neighbors, are used in many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1.8, 2.0, -3.7, 4.7])\n",
    "v = np.array([0.6, 2.1, 1.9, -1.4])\n",
    "w = np.array([2.0, 1.9, -4, 4.6])\n",
    "\n",
    "print(f\"The distance between u and v is: {npl.norm(u - v):.2f}\")\n",
    "print(f\"The distance between u and w is: {npl.norm(u - w):.2f}\")\n",
    "print(f\"The distance between v and w is: {npl.norm(v - w):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this notion of distance to create a \"nearest neighbor\" function:\n",
    "def nearest_neighbor(x, z):\n",
    "    return z[np.argmin([npl.norm(x - y) for y in z])]\n",
    "\n",
    "\n",
    "z = ([2, 1], [7, 2], [5.5, 4], [4, 8], [1, 5], [9, 6])\n",
    "\n",
    "pointa, pointb = [5, 6], [3, 3]\n",
    "\n",
    "print(\"nearest neighbours of z to the point a: \", nearest_neighbor(np.array(pointa), z))\n",
    "print(\"nearest neighbours of z to the point b: \", nearest_neighbor(np.array(pointb), z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.scatter(*zip(*z))\n",
    "# print(*zip(*z))\n",
    "n = [str(i) for i in z]\n",
    "\n",
    "for i, txt in enumerate(n):\n",
    "    plt.annotate(txt, (z[i][0] + 0.1, z[i][1] + 0.1))\n",
    "\n",
    "plt.annotate(\"point a\", map(lambda x: x + 0.1, pointa))\n",
    "plt.annotate(\"point b\", map(lambda x: x + 0.1, pointb))\n",
    "\n",
    "plt.scatter(pointa[0], pointa[1])\n",
    "plt.scatter(pointb[0], pointb[1])\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation\n",
    "\n",
    "For any vector $x$, the vector $$\\tilde{x}=x-\\operatorname{avg}(x) \\mathbf{1}$$ is called the associated **de-meaned** vector, obtained by subtracting from each entry of $x$ the mean value of the entries. (This is not standard notation; i.e., $\\tilde{x}$ is not generally used to denote the de-meaned vector.) \n",
    "\n",
    "* The mean value of the entries of $\\tilde{x}$ is zero, i.e., $\\operatorname{avg}(\\tilde{x})=0$. This explains why $\\tilde{x}$ is called the de-meaned version of $x$; it is $x$ with its mean removed. \n",
    "\n",
    "* The de-meaned vector is useful for understanding how the entries of a vector deviate from their mean value. It is zero if all the entries in the original vector $x$ are the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-meaning is useful for understanding how entries of a vector deviate\n",
    "# from the mean also gives us SD in terms of norm\n",
    "def de_mean(x):\n",
    "    return x - np.average(x)\n",
    "\n",
    "\n",
    "x = np.array([1, -2.2, 3])\n",
    "\n",
    "print(f\"the average of x: {np.average(x)}\")\n",
    "print(f\"the de-mean of x: {de_mean(x)}\")\n",
    "print(f\"the average of the de-mean of x: {np.average(de_mean(x)).round()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **standard deviation** of an $n$-vector $x$ is defined as the RMS value of the de-meaned vector $x-\\operatorname{avg}(x) \\mathbf{1}$, i.e.,\n",
    "$$\n",
    "\\operatorname{std}(x)=\\sqrt{\\frac{\\left(x_1-\\operatorname{avg}(x)\\right)^2+\\cdots+\\left(x_n-\\operatorname{avg}(x)\\right)^2}{n}}.\n",
    "$$\n",
    "This is the same as the RMS deviation between a vector $x$ and the vector all of whose entries are $\\operatorname{avg}(x)$. It can be written using the inner product and norm as\n",
    "$$\n",
    "\\operatorname{std}(x)=\\frac{\\left\\|x-\\left(\\mathbf{1}^T x / n\\right) \\mathbf{1}\\right\\|}{\\sqrt{n}}.\n",
    "$$\n",
    "\n",
    "* The standard deviation of a vector $x$ tells us the typical amount by which its entries deviate from their average value. \n",
    "* The standard deviation of a vector is zero only when all its entries are equal. \n",
    "* The standard deviation of a vector is small when the entries of the vector are nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation is RMS of a de-meaned vector\n",
    "# gives the typical amount that vector values deviate from mean\n",
    "x = np.random.rand(100)\n",
    "\n",
    "\n",
    "def stdev(x):\n",
    "    return npl.norm(x - np.average(x)) / np.sqrt(len(x))\n",
    "\n",
    "\n",
    "print(f\"the standard deviation of x: {stdev(x):.3f}\")\n",
    "# print(f\"the standard deviation of x: {np.std(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average, RMS value, and standard deviation\n",
    "\n",
    "The average, RMS value, and standard deviation of a vector are related by the formula\n",
    "$$\n",
    "\\operatorname{rms}(x)^2=\\operatorname{avg}(x)^2+\\operatorname{std}(x)^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHS = rms(x) ** 2\n",
    "RHS = np.average(x) ** 2 + np.std(x) ** 2\n",
    "print(LHS)\n",
    "print(RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "> ##### Mean return and risk\n",
    ">\n",
    "> Suppose that an $n$-vector represents a time series of return on an investment, expressed as a percentage, in $n$ time periods over some interval of time. \n",
    "> \n",
    "> Its average gives the mean return over the whole interval, often shortened to its return. \n",
    "> \n",
    "> Its standard deviation is a measure of how variable the return is, from period to period, over the time interval, i.e., how much it typically varies from its mean, and is often called the (per period) risk of the investment. \n",
    "> \n",
    "> Multiple investments can be compared by plotting them on a risk-return plot, which gives the mean and standard deviation of the returns of each of the investments over some interval. \n",
    "> \n",
    "> A desirable return history vector has high mean return and low risk; this means that the returns in the different periods are consistently high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(10)\n",
    "print(f\"the return of a is: {np.mean(a)},\\nthe risk of a is: {np.std(a)}\")\n",
    "\n",
    "b = np.array([5, 1, -2, 3, 6, 3, -1, 3, 4, 1])\n",
    "print(f\"the return of b is: {np.mean(b)},\\nthe risk of b is: {np.std(b)}\")\n",
    "\n",
    "c = np.array([5, 7, -2, 2, -3, 1, -1, 2, 7, 8])\n",
    "print(f\"the return of c is: {np.mean(c)},\\nthe risk of c is: {np.std(c)}\")\n",
    "\n",
    "d = np.array([-1, -3, -4, -3, 7, -1, 0, 3, 9, 5])\n",
    "print(f\"the return of d is: {np.mean(d)},\\nthe risk of d is: {np.std(d)}\")\n",
    "\n",
    "investments = {\n",
    "    \"a\": a,\n",
    "    \"b\": b,\n",
    "    \"c\": c,\n",
    "    \"d\": d,\n",
    "}\n",
    "\n",
    "\n",
    "returns = []\n",
    "risks = []\n",
    "\n",
    "for item in investments.values():\n",
    "    returns.append(np.mean(item))\n",
    "    risks.append(np.std(item))\n",
    "\n",
    "fig = plt.figure()\n",
    "for key, value in investments.items():\n",
    "    plt.plot(value, \"o-\", label=key)\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(risks, returns, s=80, label=\"risk and return\")\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel(\"risk\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization.\n",
    "\n",
    "For any vector $x$, we refer to $\\tilde{x}=x-\\mathbf{a v g}(x) \\mathbf{1}$ as the de-meaned version of $x$, since it has average or mean value zero. \n",
    "\n",
    "If we then divide by the RMS value of $\\tilde{x}$ (which is the standard deviation of $x$ ), we obtain the vector\n",
    "$$\n",
    "z=\\frac{1}{\\operatorname{std}(x)}(x-\\operatorname{avg}(x) \\mathbf{1})\n",
    "$$\n",
    "\n",
    "This vector is called the **standardized version** of $x$. \n",
    "* It has mean zero, and standard deviation one. \n",
    "* Its entries are sometimes called the $z$-scores associated with the original entries of $x$. \n",
    "* For example, $z_4=1.4$ means that $x_4$ is 1.4 standard deviations above the mean of the entries of $x$.\n",
    "* The standardized values for a vector give a simple way to interpret the original values in the vectors. \n",
    "\n",
    "> For example, if an $n$-vector $x$ gives the values of some medical test of $n$ patients admitted to a hospital, the standardized values or $z$ scores tell us how high or low, compared to the population, that patient's value is. \n",
    "> \n",
    "> A value $ z_6=-3.2 $, for example, means that patient 6 has a very low value of the measurement; whereas $z_{22}=0.3$ says that patient 22's value is quite close to the average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x - np.average(x)) / rms(x - np.average(x))\n",
    "\n",
    "\n",
    "x = np.random.randint(10, size=100)\n",
    "# print(x)\n",
    "z = standardize(x)\n",
    "\n",
    "print(\n",
    "    f\"average: {np.mean(x)}\\n\"\n",
    "    f\"mean {np.std(x)}\\n\"\n",
    "    f\"standardized average: {np.mean(z).round()}\\n\"\n",
    "    f\"standardized mean: {np.std(z)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(x, label=\"original\")\n",
    "plt.plot(z, label=\"standardized\")\n",
    "plt.legend(loc=0)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angle\n",
    "\n",
    "##### Angle between vectors.\n",
    "\n",
    "The angle between two nonzero vectors $a, b$ is defined as\n",
    "$$\n",
    "\\theta=\\arccos \\left(\\frac{a^T b}{\\|a\\|\\|b\\|}\\right)\n",
    "$$\n",
    "where arccos denotes the inverse cosine, normalized to lie in the interval $[0, \\pi]$. In other words, we define $\\theta$ as the unique number between 0 and $\\pi$ that satisfies\n",
    "$$\n",
    "a^T b=\\|a\\|\\|b\\| \\cos \\theta .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ang(x, y):\n",
    "    return np.arccos(np.inner(x, y) / (npl.norm(x) * npl.norm(y)))\n",
    "\n",
    "\n",
    "a = [1, 2, -1]\n",
    "b = [2, 0, -3]\n",
    "\n",
    "print(f\"the angle between a and b is: {ang(a, b):.2f} radians\")\n",
    "print(f\"the angle between a and b is: {ang(a, b) * (360 / (2 * np.pi)):.2f} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples\n",
    "\n",
    "> ##### Spherical distance.\n",
    ">\n",
    "> Suppose $a$ and $b$ are 3-vectors that represent two points that lie on a sphere of radius $R$ (for example, locations on earth). The spherical distance between them, measured along the sphere, is given by $R\\times\\Delta(a, b)$, where $\\Delta(a, b)$ is the angle between the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def generate_random_point_on_sphere(radius):\n",
    "    \"\"\"Generate a random point on the surface of a sphere.\"\"\"\n",
    "    theta = np.random.uniform(0, np.pi)  # Polar angle\n",
    "    phi = np.random.uniform(0, 2 * np.pi)  # Azimuthal angle\n",
    "    x = radius * np.sin(theta) * np.cos(phi)\n",
    "    y = radius * np.sin(theta) * np.sin(phi)\n",
    "    z = radius * np.cos(theta)\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "\n",
    "def spherical_distance(a, b, radius):\n",
    "    \"\"\"Calculate the spherical distance between two points on a sphere.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    angle = np.arccos(dot_product / (radius**2))\n",
    "    return radius * angle\n",
    "\n",
    "\n",
    "def euclidean_distance(a, b):\n",
    "    \"\"\"Calculate the Euclidean distance between two points.\"\"\"\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "\n",
    "def plot_sphere_and_points(a, b, radius):\n",
    "    \"\"\"Visualize the sphere and the two points.\"\"\"\n",
    "    # Create a sphere\n",
    "    u = np.linspace(0, 2 * np.pi, 100)\n",
    "    v = np.linspace(0, np.pi, 100)\n",
    "    x = radius * np.outer(np.cos(u), np.sin(v))\n",
    "    y = radius * np.outer(np.sin(u), np.sin(v))\n",
    "    z = radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.plot_surface(x, y, z, color=\"lightblue\", alpha=0.5)\n",
    "\n",
    "    # Plot points\n",
    "    ax.scatter(*a, color=\"red\", s=100, label=\"Point A\")\n",
    "    ax.scatter(*b, color=\"blue\", s=100, label=\"Point B\")\n",
    "\n",
    "    # Labels and legend\n",
    "    ax.set_xlabel(\"X-axis\")\n",
    "    ax.set_ylabel(\"Y-axis\")\n",
    "    ax.set_zlabel(\"Z-axis\")\n",
    "    ax.set_title(\"Spherical Distance Visualization\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "R = 1  # Radius of the sphere\n",
    "point_a = generate_random_point_on_sphere(R)\n",
    "point_b = generate_random_point_on_sphere(R)\n",
    "\n",
    "distance = spherical_distance(point_a, point_b, R)\n",
    "e_distance = euclidean_distance(point_a, point_b)\n",
    "print(f\"Spherical Distance between points A and B: {distance:.4f}\")\n",
    "print(f\"Euclidean Distance between points A and B: {e_distance:.4f}\")\n",
    "\n",
    "plot_sphere_and_points(point_a, point_b, R)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(x, y):\n",
    "    dot_product = np.dot(x, y)\n",
    "    magnitude_x = np.linalg.norm(x)\n",
    "    magnitude_y = np.linalg.norm(y)\n",
    "    cos_similarity = dot_product / (magnitude_x * magnitude_y)\n",
    "    angle = np.arccos(cos_similarity)\n",
    "    return angle\n",
    "\n",
    "\n",
    "def plot_vectors(x, y):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.quiver(\n",
    "        0,\n",
    "        0,\n",
    "        x[0],\n",
    "        x[1],\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"r\",\n",
    "        label=\"Document 1\",\n",
    "    )\n",
    "    plt.quiver(\n",
    "        0,\n",
    "        0,\n",
    "        y[0],\n",
    "        y[1],\n",
    "        angles=\"xy\",\n",
    "        scale_units=\"xy\",\n",
    "        scale=1,\n",
    "        color=\"b\",\n",
    "        label=\"Document 2\",\n",
    "    )\n",
    "\n",
    "    # Set limits and labels\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.axhline(0, color=\"black\", linewidth=0.5, ls=\"--\")\n",
    "    plt.axvline(0, color=\"black\", linewidth=0.5, ls=\"--\")\n",
    "    plt.grid()\n",
    "    plt.title(\"Vector Representation of Documents\")\n",
    "    plt.xlabel(\"Word Count Dimension 1\")\n",
    "    plt.ylabel(\"Word Count Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example word counts for two documents\n",
    "doc1 = np.array([5, 10])  # Document 1 word counts\n",
    "doc2 = np.array([10, 5])  # Document 2 word counts\n",
    "\n",
    "# Calculate the angle between the two documents\n",
    "angle = calculate_angle(doc1, doc2)\n",
    "print(f\"The angle between the two documents is: {np.degrees(angle):.2f} degrees\")\n",
    "\n",
    "# Plot the vectors\n",
    "plot_vectors(doc1, doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "def create_word_count_vector(doc1, doc2):\n",
    "    # Combine documents to create a vocabulary\n",
    "    words = re.findall(r\"\\w+\", doc1.lower()) + re.findall(r\"\\w+\", doc2.lower())\n",
    "    vocabulary = set(words)\n",
    "\n",
    "    # Count words in each document\n",
    "    count1 = Counter(re.findall(r\"\\w+\", doc1.lower()))\n",
    "    count2 = Counter(re.findall(r\"\\w+\", doc2.lower()))\n",
    "\n",
    "    # Create vectors based on the vocabulary\n",
    "    vector1 = np.array([count1[word] for word in vocabulary])\n",
    "    vector2 = np.array([count2[word] for word in vocabulary])\n",
    "\n",
    "    return vector1, vector2\n",
    "\n",
    "\n",
    "# Example documents\n",
    "doc1 = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\"\"\"\n",
    "doc2 = \"\"\"Lorem ipsum dolor amet, consectetur adipiscing elit. Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit.\"\"\"\n",
    "\n",
    "# Create word count vectors\n",
    "vector1, vector2 = create_word_count_vector(doc1, doc2)\n",
    "\n",
    "# Calculate the angle between the two documents\n",
    "angle = calculate_angle(vector1, vector2)\n",
    "print(f\"The angle between the two documents is: {np.degrees(angle):.2f} degrees\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation coefficient.\n",
    "\n",
    "Suppose $a$ and $b$ are $n$-vectors, with associated de-meaned vectors\n",
    "$$\n",
    "\\tilde{a}=a-\\operatorname{avg}(a) \\mathbf{1}, \\quad \\tilde{b}=b-\\operatorname{avg}(b) \\mathbf{1}.\n",
    "$$\n",
    "\n",
    "Assuming these de-meaned vectors are not zero (which occurs when the original vectors have all equal entries), we define their correlation coefficient as\n",
    "$$\n",
    "\\rho=\\frac{\\tilde{a}^T \\tilde{b}}{\\|\\tilde{a}\\|\\|\\tilde{b}\\|}.\n",
    "$$\n",
    "\n",
    "Thus, $\\rho=\\cos \\theta$, where $\\theta=\\angle(\\tilde{a}, \\tilde{b})$. We can also express the correlation coefficient in terms of the vectors $u$ and $v$ obtained by standardizing $a$ and $b$. With $u=\\tilde{a} / \\operatorname{std}(a)$ and $v=\\tilde{b} / \\operatorname{std}(b)$, we have\n",
    "$$\n",
    "\\rho=u^T v / n.\n",
    "$$\n",
    "(We use $\\|u\\|=\\|v\\|=\\sqrt{n}$.)\n",
    "\n",
    "* This is a symmetric function of the vectors: \n",
    "  The correlation coefficient between $a$ and $b$ is the same as the correlation coefficient between $b$ and $a$.\n",
    "  \n",
    "* The CauchySchwarz inequality tells us that the correlation coefficient ranges between $-1$ and $+1$. \n",
    "* For this reason, the correlation coefficient is sometimes expressed as a percentage. \n",
    "* For example, $\\rho=30 \\%$ means $\\rho=0.3$. When $\\rho=0$, we say the vectors are uncorrelated. (By convention, we say that a vector with all entries equal is uncorrelated with any vector.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correl_coef(a, b):\n",
    "    a_tilde = a - np.average(a)\n",
    "    b_tilde = b - np.average(b)\n",
    "    return (np.inner(a_tilde, b_tilde)) / (npl.norm(a_tilde) * npl.norm(b_tilde))\n",
    "\n",
    "\n",
    "a = np.array([4.4, 9.4, 15.4, 12.4, 10.4, 1.4, -4.6, -5.6, -0.6, 7.4])\n",
    "b = np.array([6.2, 11.2, 14.2, 14.2, 8.2, 2.2, -3.8, -4.8, -1.8, 4.2])\n",
    "\n",
    "# a = np.array([4.1, 10.1, 15.1, 13.1, 7.1, 2.1, -2.9, -5.9, 0.1, 7.1])\n",
    "# b = np.array([5.5, -0.5, -4.5, -3.5, 1.5, 7.5, 13.5, 14.5, 11.5, 4.5])\n",
    "\n",
    "# a = np.array([-5.0, 0.0, 5.0, 8.0, 13.0, 11.0, 1.0, 6.0, 4.0, 7.0])\n",
    "# b = np.array([5.8, 0.8, 7.8, 9.8, 0.8, 11.8, 10.8, 5.8, -0.2, -3.2])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(a)), a, \"-o\", label=\"a\")\n",
    "plt.plot(range(len(b)), b, \"-o\", label=\"b\")\n",
    "plt.xlabel(\"a\")\n",
    "plt.ylabel(\"b\")\n",
    "plt.title(f\"correlation coefficient: {correl_coef(a, b):.4f}\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity\n",
    "\n",
    "* Computing the norm of an $n$-vector requires $n$ multiplications (to square each entry), $n-1$ additions (to add the squares), and one squareroot. Even though computing the squareroot typically takes more time than computing the product or sum of two numbers, it is counted as just one flop. So computing the norm takes $2 n$ flops.\n",
    "\n",
    "* The cost of computing the RMS value of an $n$-vector is the same, since we can ignore the two flops involved in division by $\\sqrt{n}$.\n",
    "\n",
    "* Computing the distance between two vectors costs $3 n$ flops, and computing the angle between them costs $6 n$ flops.\n",
    "\n",
    "All of these operations have order $n$.\n",
    "\n",
    "* De-meaning an $n$-vector requires $2 n$ flops ( $n$ for forming the average and another $n$ flops for subtracting the average from each entry).\n",
    "\n",
    "* The standard deviation is the RMS value of the de-meaned vector, and this calculation takes $4 n$ flops ( $2 n$ for computing the de-meaned vector and $2 n$ for computing its RMS value).\n",
    "\n",
    "* We can suggests a slightly more efficient method with a complexity of $3 n$ flops: first compute the average ( $n$ flops) and RMS value ( $2 n$ flops), and then find the standard deviation as $\\operatorname{std}(x)=\\left(\\mathbf{r m s}(x)^2-\\mathbf{a v g}(x)^2\\right)^{1 / 2}$.\n",
    "* Standardizing an $n$-vector costs $5 n$ flops.\n",
    "* The correlation coefficient between two vectors costs $10 n$ flops to compute.\n",
    "\n",
    "These operations also have order $n$.\n",
    "\n",
    "* As a slightly more involved computation, suppose that we wish to determine the nearest neighbor among a collection of $k$ $n$-vectors $z_1, \\ldots, z_k$ to another $n$-vector $x$. (This will come up in the next chapter.) The simple approach is to compute the distances $\\left\\|x-z_i\\right\\|$ for $i=1, \\ldots, k$, and then find the minimum of these. (Sometimes a comparison of two numbers is also counted as a flop.) The cost of this is $3 \\mathrm{kn}$ flops to compute the distances, and $k-1$ comparisons to find the minimum. The latter term can be ignored, so the flop count is $3 \\mathrm{kn}$. The order of finding the nearest neighbor in a collection of $k$ $n$-vectors is $k n$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
