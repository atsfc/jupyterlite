{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Linear independence\n",
        "\n",
        "### Linear dependence\n",
        "\n",
        "A collection or list of $n$-vectors $a_{1},\\ldots,a_{k}$ (with $k\\geq 1$) is called _linearly dependent_ if\n",
        "\n",
        "$$ \\beta_{1}a_{1}+\\cdots+\\beta_{k}a_{k}=0 $$\n",
        "\n",
        "holds for some $\\beta_{1},\\ldots,\\beta_{k}$ that are not all zero. In other words, we can form the zero vector as a linear combination of the vectors, with coefficients that are not all zero. Linear dependence of a list of vectors does not depend on the ordering of the vectors in the list.\n",
        "\n",
        "#### Linear combinations of linearly independent vectors.\n",
        "\n",
        "Suppose a vector $x$ is a linear combination of $a_1, \\ldots, a_k$,\n",
        "$$\n",
        "x=\\beta_1 a_1+\\cdots+\\beta_k a_k .\n",
        "$$\n",
        "\n",
        "When the vectors $a_1, \\ldots, a_k$ are linearly independent, the coefficients that form $x$ are unique: If we also have\n",
        "$$\n",
        "x=\\gamma_1 a_1+\\cdots+\\gamma_k a_k\n",
        "$$\n",
        "then $\\beta_i=\\gamma_i$ for $i=1, \\ldots, k$. This tells us that, in principle at least, we can find the coefficients that form a vector $x$ as a linear combination of linearly independent vectors."
      ],
      "metadata": {
        "id": "OCyfQONbUj1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basis\n",
        "#### Independence-dimension inequality.\n",
        "If the $n$-vectors $a_1, \\ldots, a_k$ are linearly independent, then $k \\leq n$. In words:\n",
        "> A linearly independent collection of $n$-vectors can have at most $n$ elements.\n",
        "\n",
        "Put another way:\n",
        ">Any collection of $n+1$ or more n-vectors is linearly dependent.\n",
        "\n",
        "As a very simple example, we can conclude that any three 2 -vectors must be linearly dependent.\n",
        "We will prove this fundamental fact below; but first, we describe the concept of basis, which relies on the independence-dimension inequality.\n",
        "\n",
        "#### Basis.\n",
        "\n",
        "A collection of $n$ linearly independent $n$-vectors (i.e., a collection of linearly independent vectors of the maximum possible size) is called a **basis**.\n",
        "\n",
        "If the $n$-vectors $a_1, \\ldots, a_n$ are a basis, then any $n$-vector $b$ can be written as a linear combination of them. To see this, consider the collection of $n+1$ $n$-vectors $a_1, \\ldots, a_n, b$. By the independence-dimension inequality, these vectors are linearly dependent, so there are $\\beta_1, \\ldots, \\beta_{n+1}$, not all zero, that satisfy\n",
        "$$\n",
        "\\beta_1 a_1+\\cdots+\\beta_n a_n+\\beta_{n+1} b=0\n",
        "$$\n",
        "\n",
        "If $\\beta_{n+1}=0$, then we have\n",
        "$$\n",
        "\\beta_1 a_1+\\cdots+\\beta_n a_n=0\n",
        "$$\n",
        "which, since $a_1, \\ldots, a_n$ are linearly independent, implies that $\\beta_1=\\cdots=\\beta_n=0$. But then all the $\\beta_i$ are zero, a contradiction. So we conclude that $\\beta_{n+1} \\neq 0$. It follows that\n",
        "$$\n",
        "b=\\left(-\\beta_1 / \\beta_{n+1}\\right) a_1+\\cdots+\\left(-\\beta_n / \\beta_{n+1}\\right) a_n\n",
        "$$\n",
        "i.e., $b$ is a linear combination of $a_1, \\ldots, a_n$.\n",
        "\n",
        "Combining this result with the observation above that any linear combination of linearly independent vectors can be expressed in only one way, we conclude:\n",
        "\n",
        "> Any $n$-vector $b$ can be written in a unique way as a linear combination of a basis $a_1, \\ldots, a_n$.\n",
        "\n",
        "#### Expansion in a basis.\n",
        "\n",
        "When we express an $n$-vector $b$ as a linear combination of a basis $a_1, \\ldots, a_n$, we refer to\n",
        "$$\n",
        "b=\\alpha_1 a_1+\\cdots+\\alpha_n a_n\n",
        "$$\n",
        "as the expansion of $b$ in the $a_1, \\ldots, a_n$ basis. The numbers $\\alpha_1, \\ldots, \\alpha_n$ are called the coefficients of the expansion of $b$ in the basis $a_1, \\ldots, a_n$."
      ],
      "metadata": {
        "id": "SLoJ2NbUcQs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Examples\n",
        "\n",
        "- The $n$ standard unit $n$ vectors $e_1, \\ldots, e_n$ are a basis. Any $n$-vector $b$ can be written as the linear combination\n",
        "$$\n",
        "b=b_1 e_1+\\cdots+b_n e_n .\n",
        "$$\n",
        "This expansion is unique, which means that there is no other linear combination of $e_1, \\ldots, e_n$ that equals $b$.\n",
        "- The vectors\n",
        "$$\n",
        "a_1=\\left[\\begin{array}{r}\n",
        "1.2 \\\\\n",
        "-2.6\n",
        "\\end{array}\\right], \\quad a_2=\\left[\\begin{array}{l}\n",
        "-0.3 \\\\\n",
        "-3.7\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "are a basis. The vector $b=(1,1)$ can be expressed in only one way as a linear combination of them:\n",
        "$$\n",
        "b=0.6513 a_1-0.7280 a_2 .\n",
        "$$\n",
        "(The coefficients are given here to 4 significant digits. We will see later how these coefficients can be computed.)"
      ],
      "metadata": {
        "id": "n6STahm3ebot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orthonormal vectors\n",
        "\n",
        "A collection of vectors $a_1, \\ldots, a_k$ is orthogonal or mutually orthogonal if $a_i \\perp a_j$ for any $i, j$ with $i \\neq j, i, j=1, \\ldots, k$. A collection of vectors $a_1, \\ldots, a_k$ is orthonormal if it is orthogonal and $\\left\\|a_i\\right\\|=1$ for $i=1, \\ldots, k$. (A vector of norm one is called normalized; dividing a vector by its norm is called normalizing it.) Thus, each vector in an orthonormal collection of vectors is normalized, and two different vectors from the collection are orthogonal. These two conditions can be combined into one statement about the inner products of pairs of vectors in the collection: $a_1, \\ldots, a_k$ is orthonormal means that\n",
        "$$\n",
        "a_i^T a_j= \\begin{cases}1 & i=j \\\\ 0 & i \\neq j .\\end{cases}\n",
        "$$\n",
        "\n",
        "Orthonormality, like linear dependence and independence, is an attribute of a collection of vectors, and not an attribute of vectors individually. By convention, though, we say \"The vectors $a_1, \\ldots, a_k$ are orthonormal\" to mean \"The collection of vectors $a_1, \\ldots, a_k$ is orthonormal\"."
      ],
      "metadata": {
        "id": "hMkz4qDBfGJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as npl\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4zb1btXzcP50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a1 = np.array([0, 0, -1])\n",
        "a2 = np.array([1, 1, 0]) / np.sqrt(2)\n",
        "a3 = np.array([1, -1, 0]) / np.sqrt(2)\n",
        "\n",
        "print(\n",
        "    f\"norm a1: {npl.norm(a1)}, \\nnorm a2: {npl.norm(a2)}, \\nnorm a3: {npl.norm(a3)}\\n\"\n",
        "    f\"inner product a1a2: {np.inner(a1, a2)}, \\ninner product a1a3: {np.inner(a1, a3)}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxWqcHNiR59",
        "outputId": "85e3ab8a-9c9f-4a14-81c0-fffe63510ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm a1: 1.0, \n",
            "norm a2: 0.9999999999999999, \n",
            "norm a3: 0.9999999999999999\n",
            "inner product a1a2: 0.0, \n",
            "inner product a1a3: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3])\n",
        "\n",
        "beta1 = np.inner(a1, x)\n",
        "beta2 = np.inner(a2, x)\n",
        "beta3 = np.inner(a3, x)\n",
        "\n",
        "xexp = beta1 * a1 + beta2 * a2 + beta3 * a3\n",
        "print(\n",
        "    f\"expansion {xexp}\\n\"\n",
        "    f\"original {x}\\n\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89hFO3e9ibTi",
        "outputId": "51401b1f-5e65-45bc-997c-f22fe845a478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expansion [1. 2. 3.]\n",
            "original [1 2 3]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gram–Schmidt algorithm\n",
        "\n",
        "**Algorithm:** given $n$-vectors $a_1, \\ldots, a_k$ for $i=1, \\ldots, k$,\n",
        "1. Orthogonalization. $\\tilde{q}_i=a_i-\\left(q_1^T a_i\\right) q_1-\\cdots-\\left(q_{i-1}^T a_i\\right) q_{i-1}$\n",
        "2. Test for linear dependence. if $\\tilde{q}_i=0$, quit.\n",
        "3. Normalization. $q_i=\\tilde{q}_i /\\left\\|\\tilde{q}_i\\right\\|$"
      ],
      "metadata": {
        "id": "XbGfvt_ic_sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_schmidt(a, tol=1e-10):\n",
        "    q = []\n",
        "    for i in range(len(a)):\n",
        "        qtilde = a[i]\n",
        "\n",
        "        for j in range(i):\n",
        "            qtilde = qtilde - np.inner(q[j], a[i]) * q[j]\n",
        "\n",
        "        if npl.norm(qtilde) < tol:\n",
        "            print(\"Vectors linearly dependent\")\n",
        "\n",
        "        q.append(qtilde / npl.norm(qtilde))\n",
        "    return q"
      ],
      "metadata": {
        "id": "MRR2qJt0e40u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[-1, 1, -1, 1], [-1, 3, -1, 3], [1, 3, 5, 7]])\n",
        "q = gram_schmidt(a)\n",
        "print(f\"The orthonormal vectors are:\\n{q}\")\n",
        "print(f\"Norms of vectors {np.linalg.norm(q, axis=1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E16G2I3jfraB",
        "outputId": "0c5f7f30-abcc-4fe7-ca22-bfa58bfebe81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The orthonormal vectors are:\n",
            "[[-0.5         0.5        -0.5         0.5       ]\n",
            " [ 0.57539646 -0.41099747  0.57539646 -0.41099747]\n",
            " [ 0.58171796 -0.39099076  0.6198634  -0.35284532]]\n",
            "Norms of vectors [1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrices\n"
      ],
      "metadata": {
        "id": "h4NtoUp4irEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.matrix([[0.0, 1.0, -2.3, 0.1], [1.3, 4.0, -0.1, 0.0],[4.1,-1.0, 0.0, 1.7]])\n",
        "B = np.array([[0.0, 1.0, -2.3, 0.1], [1.3, 4.0, -0.1, 0.0],[4.1,-1.0, 0.0, 1.7]])\n",
        "print(type(A))\n",
        "print(type(B))\n",
        "print(np.array_equal(A, B))\n",
        "B[2][0] = 0.\n",
        "print(np.array_equal(A, B))\n",
        "A == B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yFJorKBi-KD",
        "outputId": "540f3d49-fb6d-407e-d946-47e2d00d1b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.matrix'>\n",
            "<class 'numpy.ndarray'>\n",
            "True\n",
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ True,  True,  True,  True],\n",
              "        [ True,  True,  True,  True],\n",
              "        [False,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A[:,2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwqkSaIzi_6T",
        "outputId": "b082a00a-a0e8-4432-ed1e-e172927fc542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[-2.3],\n",
              "        [-0.1],\n",
              "        [ 0. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.flatten()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AJCOQuClvqs",
        "outputId": "1c151cfa-dd5f-41e7-a58c-68bf054ac4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0. ,  1. , -2.3,  0.1,  1.3,  4. , -0.1,  0. ,  4.1, -1. ,\n",
              "          0. ,  1.7]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.reshape(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsrMG5kskcm-",
        "outputId": "9b72134e-e9ec-42b0-f3b5-c324fa6d5c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0. ,  1. , -2.3,  0.1,  1.3,  4. , -0.1,  0. ,  4.1, -1. ,\n",
              "          0. ,  1.7]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.reshape(4, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "falXLPQ4lZGZ",
        "outputId": "39cf287b-c330-4ae4-9f44-a7ab59678839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0. ,  1. , -2.3],\n",
              "        [ 0.1,  1.3,  4. ],\n",
              "        [-0.1,  0. ,  4.1],\n",
              "        [-1. ,  0. ,  1.7]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((A, A[:, 0]), axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyfiG2EaklGe",
        "outputId": "3859e36c-591d-4f9c-f4dd-b0ec8be235df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0. ,  1. , -2.3,  0.1,  0. ],\n",
              "        [ 1.3,  4. , -0.1,  0. ,  1.3],\n",
              "        [ 4.1, -1. ,  0. ,  1.7,  4.1]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.block([A, B])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_0jNKoklKyw",
        "outputId": "13bc06d6-b700-4e13-f1f6-168dfe26e33f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0. ,  1. , -2.3,  0.1,  0. ,  1. , -2.3,  0.1],\n",
              "        [ 1.3,  4. , -0.1,  0. ,  1.3,  4. , -0.1,  0. ],\n",
              "        [ 4.1, -1. ,  0. ,  1.7,  0. , -1. ,  0. ,  1.7]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KfGStany8Z8",
        "outputId": "0bb9fa57-c3d4-4a2e-bbd8-518a2e48cdc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -2],\n",
              "       [ 2,  1],\n",
              "       [-1,  1]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23lkCPIQzEQC",
        "outputId": "9c2635f5-d77b-4acb-a7ee-c360cb9df5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.3166247903554"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero and identity matrices"
      ],
      "metadata": {
        "id": "C0L4Coi9nR5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.zeros((2,2))\n",
        "# A = np.identity(4)\n",
        "# A = np.diag([1,2,3])\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TknC8BJfnbhc",
        "outputId": "84231e76-0411-4afc-955c-2cff3d37866f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sparse matrices.\n",
        "\n",
        "A matrix $A$ is said to be sparse if many of its entries are zero, or (put another way) just a few of its entries are nonzero. Its sparsity pattern is the set of indices $(i, j)$ for which $A_{i j} \\neq 0$. The number of nonzeros of a sparse matrix $A$ is the number of entries in its sparsity pattern, and denoted $\\mathbf{n n z}(A)$. If $A$ is $m \\times n$ we have $\\mathbf{n n z}(A) \\leq m n$. Its density is $\\mathbf{n n z}(A) /(m n)$, which is no more than one. Densities of sparse matrices that arise in applications are typically small or very small, as in $10^{-2}$ or $10^{-4}$.\n",
        "\n",
        "There is no precise definition of how small the density must be for a matrix to qualify as sparse. A famous definition of sparse matrix due to the mathematician James H. Wilkinson is: A matrix is sparse if it has enough zero entries that it pays to take advantage of them. Sparse matrices can be stored and manipulated efficiently on a computer."
      ],
      "metadata": {
        "id": "F-2lxTozl_j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csc_matrix\n",
        "\n",
        "row_pos = np.array([0, 2, 2, 0, 1, 2])\n",
        "col_pos = np.array([0, 0, 1, 2, 2, 2])\n",
        "data = np.array([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "csc_matrix((data, (row_pos, col_pos)), shape=(3, 3)).nnz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bubkq78MoUMm",
        "outputId": "5753e338-fbaf-4a63-a197-eb9b3ef72859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix-vector multiplication\n",
        "\n"
      ],
      "metadata": {
        "id": "YP2y0bwYlJgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[0, 2, -1], [-2, 1, 1]])\n",
        "x = np.array([2, 1, -1])\n",
        "np.matmul(A, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrddIDsJvDwV",
        "outputId": "ba48d2e8-f408-4eff-fea8-64df07f28485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, -4])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Application examples\n",
        "\n",
        "> __Feature matrix and weight vector.__ Suppose $X$ is a feature matrix, where its $N$ columns $x_1, \\ldots, x_N$ are feature $n$-vectors for $N$ objects or examples. Let the $n$-vector $w$ be a weight vector, and let $s_i=x_i^T w$ be the score associated with object $i$ using the weight vector $w$. Then we can write $s=X^T w$, where $s$ is the $N$-vector of scores of the objects.\n",
        "\n",
        "> __Polynomial evaluation at multiple points.__ Suppose the entries of the $n$-vector $c$ are the coefficients of a polynomial $p$ of degree $n-1$ or less:\n",
        "$$\n",
        "p(t)=c_1+c_2 t+\\cdots+c_{n-1} t^{n-2}+c_n t^{n-1} .\n",
        "$$\n",
        ">\n",
        "> Let $t_1, \\ldots, t_m$ be $m$ numbers, and define the $m$-vector $y$ as $y_i=p\\left(t_i\\right)$. Then we have $y=A c$, where $A$ is the $m \\times n$ matrix\n",
        "$$\n",
        "A=\\left[\\begin{array}{ccccc}\n",
        "1 & t_1 & \\cdots & t_1^{n-2} & t_1^{n-1} \\\\\n",
        "1 & t_2 & \\cdots & t_2^{n-2} & t_2^{n-1} \\\\\n",
        "\\vdots & \\vdots & & \\vdots & \\vdots \\\\\n",
        "1 & t_m & \\cdots & t_m^{n-2} & t_m^{n-1}\n",
        "\\end{array}\\right]\n",
        "$$\n",
        ">\n",
        "> So multiplying a vector $c$ by the matrix $A$ is the same as evaluating a polynomial with coefficients $c$ at $m$ points. The matrix $A$ in (6.7) comes up often, and is called a Vandermonde matrix (of degree $n-1$, at the points $t_1, \\ldots, t_m$ ), named for the mathematician Alexandre-Théophile Vandermonde."
      ],
      "metadata": {
        "id": "ujD-2pCWw6mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1, 2, 3, 5])\n",
        "N = 3\n",
        "B = np.vander(x, N)\n",
        "B[:, -1::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtMDD68FwFse",
        "outputId": "20a6f871-b739-4d3b-f1eb-2811025b3462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  1,  1],\n",
              "       [ 1,  2,  4],\n",
              "       [ 1,  3,  9],\n",
              "       [ 1,  5, 25]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> __Total price from multiple suppliers.__ Suppose the $m \\times n$ matrix $P$ gives the prices of $n$ goods from $m$ suppliers (or in $m$ different locations). If $q$ is an $n$-vector of quantities of the $n$ goods (sometimes called a basket of goods), then $c=P q$ is an $N$-vector that gives the total cost of the goods, from each of the $N$ suppliers.\n",
        "\n",
        "> Document scoring. Suppose $A$ in an $N \\times n$ document-term matrix, which gives the word counts of a corpus of $N$ documents using a dictionary of $n$ words, so the rows of $A$ are the word count vectors for the documents. Suppose that $w$ in an $n$-vector that gives a set of weights for the words in the dictionary. Then $s=A w$ is an $N$-vector that gives the scores of the documents, using the weights and the word counts. A search engine, for example, might choose $w$ (based on the search query) so that the scores are predictions of relevance of the documents (to the search).\n",
        "\n",
        "> Audio mixing. Suppose the $k$ columns of $A$ are vectors representing audio signals or tracks of length $T$, and $w$ is a $k$-vector. Then $b=A w$ is a $T$-vector representing the mix of the audio signals, with track weights given by the vector $w$."
      ],
      "metadata": {
        "id": "eL_uyAF0yIc3"
      }
    }
  ]
}